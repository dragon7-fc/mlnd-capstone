{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project\n",
    "\n",
    "## Project: Real time object detection magnifier\n",
    "---\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "I break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Environment Setup\n",
    "* [Step 1](#step1): Train MobileNetv1 with EgoHands to generate CoreML model\n",
    "* [Step 2](#step2): Train MobileNetv1 with MS-COCO to generate CoreML model\n",
    "* [Step 3](#step3): Train YOLOv2 with MS-COCO to generate CoreML model\n",
    "* [Step 4](#step4): Train Tiny YOLO with MS-COCO to generate CoreML model\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Environment Setup\n",
    "\n",
    "### Python3 Kernel\n",
    "\n",
    "Change kernel to Python3 (***Kernel > Change kernel > Python3***)\n",
    "\n",
    "### The Egohands Dataset\n",
    "\n",
    "The hand detector model is built using data from the [Egohands Dataset](http://vision.soic.indiana.edu/projects/egohands/) dataset. This dataset works well for several reasons. It contains high quality, pixel level annotations (>15000 ground truth labels) where hands are located across 4800 images. All images are captured from an egocentric view (Google glass) across 48 different environments (indoor, outdoor) and activities (playing cards, chess, jenga, solving puzzles etc).\n",
    "\n",
    "<p float=\"left\">\n",
    "<img src=\"images/egohands_0.png\" width=\"180\" />\n",
    "<img src=\"images/egohands_1.png\" width=\"180\" />\n",
    "<img src=\"images/egohands_2.png\" width=\"180\" />\n",
    "</p>\n",
    "\n",
    "The Egohands dataset (zip file with labeled data) contains 48 folders of locations where video data was collected (100 images per folder).\n",
    "```\n",
    "-- LOCATION_X\n",
    "  -- frame_1.jpg\n",
    "  -- frame_2.jpg\n",
    "  ...\n",
    "  -- frame_100.jpg\n",
    "  -- polygons.mat  // contains annotations for all 100 images in current folder\n",
    "-- LOCATION_Y\n",
    "  -- frame_1.jpg\n",
    "  -- frame_2.jpg\n",
    "  ...\n",
    "  -- frame_100.jpg\n",
    "  -- polygons.mat  // contains annotations for all 100 images in current folder\n",
    "  ```\n",
    "\n",
    "### Setup environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_OBJECT_DETECTION_HOME = \"/root/Project/tensorflow/models/research/object_detection\"\n",
    "DARKNET_HOME = \"/root/Project/darknet\"\n",
    "DARKFLOW_HOME = \"/root/Project/darkflow\"\n",
    "\n",
    "MLND_CAPSTONE_HOME = \"/root/Project/capstone\"\n",
    "\n",
    "IMAGES_HOME = MLND_CAPSTONE_HOME + \"/images\"\n",
    "SAMPLE_HOME = MLND_CAPSTONE_HOME + \"/sample\"\n",
    "\n",
    "EGOHANDS_HOME = MLND_CAPSTONE_HOME + \"/dataset/EgoHands\"\n",
    "MSCOCO_HOME = MLND_CAPSTONE_HOME + \"/dataset/MS-COCO\"\n",
    "\n",
    "YOLOV2_EGOHANDS_HOME = MLND_CAPSTONE_HOME + \"/model/yolov2_608_egohands\"\n",
    "YOLOV2_TINY_EGOHANDS_HOME = MLND_CAPSTONE_HOME + \"/model/yolov2_tiny_608_egohands\"\n",
    "MOBILENET_EGOHANDS_HOME = MLND_CAPSTONE_HOME + \"/model/ssd_mobilenet_v1_300_egohands\"\n",
    "YOLOV2_COCO_HOME = MLND_CAPSTONE_HOME + \"/model/yolov2_608_coco\"\n",
    "YOLOV2_TINY_COCO_HOME = MLND_CAPSTONE_HOME + \"/model/yolov2_tiny_608_coco\"\n",
    "MOBILENET_COCO_HOME = MLND_CAPSTONE_HOME + \"/model/ssd_mobilenet_v1_300_coco\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup EgoHands dataset\n",
    "\n",
    "- All scripts create/support following folder structure to be able to support tensorflow as well as yolo-darknet projects:\n",
    "```\n",
    ".\n",
    "├── data\n",
    "│   ├── train \n",
    "│   │   ├── labels\n",
    "│   │   │   ├──file1.txt\n",
    "│   │   │   └── ...\n",
    "│   │   └── images\n",
    "│   │       ├──file1.jpg\n",
    "│   │       └── ...\n",
    "│   ├── eval\n",
    "│   │   └── ...\n",
    "│   │\n",
    "│   ├── train_labels.csv\n",
    "│   ├── eval_labels.csv\n",
    "│   ├── label_map.pbtxt\n",
    "│   ├── train.record\n",
    "│   ├── eval.record\n",
    "│   ├── train.txt\n",
    "│   └── eval.txt\n",
    "│   \n",
    "└── model\n",
    "```\n",
    "\n",
    "- The egohands_setup.py will do following things\n",
    "    - Downloads the egohands datasets\n",
    "    - Renames all files to include their directory names to ensure each filename is unique\n",
    "    - Splits the dataset into train (90%) and eval (10%) folders.\n",
    "    - Reads in polygons.mat for each folder, generates bounding boxes and visualizes them to ensure correctness (see image above).\n",
    "    - Once the script is done running, you should have an data/train/images and data/eval/images folder. Each of these folders should also contain a csv label document each - data/train_labels.csv, data/eval_labels.csv that can be used to generate tfrecords.\n",
    "    - The generated data/label_map.pbtxt is used for tensorflow. Below is it's content.\n",
    "```\n",
    "        item {\n",
    "          id: 1\n",
    "          name: 'hand'\n",
    "        }\n",
    "```\n",
    "\n",
    "Note: While the egohands dataset provides four separate labels for hands (own left, own right, other left, and other right), for my purpose, I am only interested in the general `hand` class and label all training data as `hand`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $EGOHANDS_HOME\n",
    "\n",
    "%run egohands_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Labels for Darknet\n",
    "\n",
    "- Now we need to generate the label files, i.e. data/train/labels/* or data/eval/labels, that Darknet uses. Darknet wants a .txt file for each image with a line for each ground truth object in the image that looks like:\n",
    "```\n",
    "    <object-class> <x> <y> <width> <height>\n",
    "```\n",
    "- The content of generated data/handsnet.data is as below\n",
    "```\n",
    "    classes = 1            \n",
    "    train = /root/Project/capstone/dataset/EgoHands/data/train.txt            \n",
    "    valid = /root/Project/capstone/dataset/EgoHands/data/eval.txt            \n",
    "    names = /root/Project/capstone/dataset/EgoHands/data/handsnet.names            \n",
    "    backup = /root/Project/capstone/dataset/EgoHands/model/yolo_backup/\n",
    "```\n",
    "- The content of generated data/handsnet.names is as below\n",
    "```\n",
    "    hand\n",
    "```\n",
    "- The content of generated data/train.txt or data/eval.txt is as below\n",
    "```\n",
    "    /root/Project/capstone/dataset/EgoHands/data/train/images/JENGA_OFFICE_S_B_frame_1914.jpg\n",
    "    /root/Project/capstone/dataset/EgoHands/data/train/images/CHESS_OFFICE_S_B_frame_0905.jpg\n",
    "    /root/Project/capstone/dataset/EgoHands/data/train/images/CHESS_LIVINGROOM_H_T_frame_1495.jpg\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $EGOHANDS_HOME\n",
    "\n",
    "%run csv_to_yolo_txt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate TFRecord for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $EGOHANDS_HOME\n",
    "\n",
    "%run csv_to_tfrecord.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pre-trained MobileNet MS-COCO sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $SAMPLE_HOME\n",
    "\n",
    "!tar -zxvf ssd_mobilenet_v1_coco_2017_11_17.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Train MobileNetv1 with EgoHands to generate CoreML model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the hand detection Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our records files are ready, we are almost ready to train the model.\n",
    "> **Note**: You don't have to do below things. All these things I have done in this docker image. Just run below cell to do it.\n",
    "\n",
    "- Download the and pre-trained [ssd_mobilenet_v1_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz) model.\n",
    "\n",
    "- Download the config file for the same model. In my case, I will download [ssd_mobilenet_v1_coco.config](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config). And make below modifications.\n",
    "\n",
    "\n",
    "     - Change the number of classes in the file according to our requirement.\n",
    "```\n",
    "    -- before\n",
    "        num_classes: 90\n",
    "    -- after\n",
    "        num_classes: 1\n",
    "```\n",
    "     - I have no good GPU then I decrease the batch_size.\n",
    "```\n",
    "    -- before\n",
    "        batch_size: 24\n",
    "    -- After\n",
    "        batch_size: 6\n",
    "```\n",
    "     - Give path to downloaded model i.e ssd_mobilenet_v1_coco; the model we decided to use in step 1.\n",
    "```    \n",
    "    -- before\n",
    "        fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\"\n",
    "    -- after\n",
    "        fine_tune_checkpoint: \"/root/Project/capstone/sample/ssd_mobilenet_v1_coco_2017_11_17/model.ckpt\"\n",
    "```\n",
    "     - Give path to train.record file.\n",
    "```    \n",
    "    -- before\n",
    "        train_input_reader: {  \n",
    "        tf_record_input_reader {   \n",
    "        input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record\"\n",
    "        }\n",
    "        label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n",
    "        }\n",
    "    -- after\n",
    "        train_input_reader: {  \n",
    "        tf_record_input_reader {   \n",
    "        input_path: \"/root/Project/capstone/dataset/EgoHangs/data/train.record\"\n",
    "        }\n",
    "        label_map_path: \"/root/Project/capstone/dataset/EgoHands/data/label_map.pbtxt\"\n",
    "        }\n",
    "```\n",
    "     - Give path for eval.record file\n",
    "```    \n",
    "    -- before\n",
    "        eval_input_reader: {  \n",
    "        tf_record_input_reader {\n",
    "        input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record\" \n",
    "        }\n",
    "        label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n",
    "        shuffle: false\n",
    "        num_readers: 1\n",
    "        }\n",
    "    -- after\n",
    "        eval_input_reader: {  \n",
    "        tf_record_input_reader {\n",
    "        input_path: \"/root/Project/capstone/dataset/EgoHands/data/eval.record\" \n",
    "        }\n",
    "        label_map_path: \"/root/Project/capstone/dataset/EgoHands/data/label_map.pbtxt\"  \n",
    "        shuffle: false\n",
    "        num_readers: 1\n",
    "        }\n",
    "```\n",
    "     - The eval image number should be 480.\n",
    "```\n",
    "    -- before\n",
    "        eval_config: {\n",
    "          num_examples: 8000\n",
    "    -- after\n",
    "        eval_config: {\n",
    "          num_examples: 480\n",
    "    ```\n",
    "\n",
    "Run below cells to train the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd $TF_OBJECT_DETECTION_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONPATH=/root/Project/tensorflow/models/research:/root/Project/tensorflow/models/research/slim::/root/Project/cocoapi/PythonAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py \\\n",
    "    --logtostderr \\\n",
    "    --train_dir=$MOBILENET_EGOHANDS_HOME/training \\\n",
    "    --pipeline_config_path=$MOBILENET_EGOHANDS_HOME/training/ssd_mobilenet_v1_coco_egohands.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=$MOBILENET_EGOHANDS_HOME/training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "<img src=\"sample/ssd_mobilenet_v1_300_egohands/images/total_loss.png\" width=\"300\" />\n",
    "<img src=\"sample/ssd_mobilenet_v1_300_egohands/images/regularization_loss.png\"  width=\"300\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the hand detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval.py \\\n",
    "    --logtostderr \\\n",
    "    --pipeline_config_path=$MOBILENET_EGOHANDS_HOME/training/ssd_mobilenet_v1_coco_egohands.config \\\n",
    "    --checkpoint_dir=$MOBILENET_EGOHANDS_HOME/training/ \\\n",
    "    --eval_dir=$MOBILENET_EGOHANDS_HOME/eval/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the eval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=$MOBILENET_EGOHANDS_HOME/eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sample/ssd_mobilenet_v1_300_egohands/images/map.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python export_inference_graph.py \\\n",
    "    --input_type image_tensor \\\n",
    "    --pipeline_config_path $MOBILENET_EGOHANDS_HOME/training/ssd_mobilenet_v1_coco_egohands.config \\\n",
    "    --trained_checkpoint_prefix $MOBILENET_EGOHANDS_HOME/training/model.ckpt-200000 \\\n",
    "    --output_directory $MOBILENET_EGOHANDS_HOME/egohands_inference_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check tensorflow model functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $TF_OBJECT_DETECTION_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'egohands_inference_graph'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MOBILENET_EGOHANDS_HOME + \"/\" + MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = EGOHANDS_HOME + \"/data/label_map.pbtxt\"\n",
    "\n",
    "NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `1`, we know that this corresponds to `hand`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of simplicity we will use only 2 images:\n",
    "# image1.jpg\n",
    "# image2.jpg\n",
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "PATH_TO_TEST_IMAGES_DIR = IMAGES_HOME\n",
    "TEST_IMAGE_PATHS = [ PATH_TO_TEST_IMAGES_DIR + '/oxfordhands_{}.jpg'.format(i) for i in range(0, 6) ]\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "<img src=\"sample/ssd_mobilenet_v1_300_egohands/images/oxfordhands_0.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/ssd_mobilenet_v1_300_egohands/images/oxfordhands_1.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/ssd_mobilenet_v1_300_egohands/images/oxfordhands_2.jpg\" width=\"300\"/>\n",
    "</p>\n",
    "<p float=\"left\">\n",
    "<img src=\"sample/ssd_mobilenet_v1_300_egohands/images/oxfordhands_3.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/ssd_mobilenet_v1_300_egohands/images/oxfordhands_4.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/ssd_mobilenet_v1_300_egohands/images/oxfordhands_5.jpg\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tensorflow model to CoreML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TF graph definition\n",
    "tf_model_path = PATH_TO_CKPT\n",
    "with open(tf_model_path, 'rb') as f:\n",
    "    serialized = f.read()\n",
    "tf.reset_default_graph()\n",
    "original_gdef = tf.GraphDef()\n",
    "original_gdef.ParseFromString(serialized)\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    tf.import_graph_def(original_gdef, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full MobileNet-SSD TF model contains 4 subgraphs: *Preprocessor*, *FeatureExtractor*, *MultipleGridAnchorGenerator*, and *Postprocessor*. Here we will extract the *FeatureExtractor* from the model and strip off the other subgraphs, as these subgraphs contain structures not currently supported in CoreML. The tasks in *Preprocessor*, *MultipleGridAnchorGenerator* and *Postprocessor* subgraphs can be achieved by other means, although they are non-trivial.\n",
    "\n",
    "By inspecting TensorFlow GraphDef, it can be found that:\n",
    "(1) the input tensor of MobileNet-SSD Feature Extractor is `Preprocessor/sub:0` of shape `(1,300,300,3)`, which contains the preprocessed image.\n",
    "(2) The output tensors are: `concat:0` of shape `(1,1917,4)`, the box coordinate encoding for each of the 1917 anchor boxes; and `concat_1:0` of shape `(1,1917,1)`, the confidence scores (logits) for each of the 1 object classes (including 1 class for background), for each of the 1917 anchor boxes.\n",
    "So we extract the feature extractor out as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip unused subgraphs and save it as another frozen TF model\n",
    "from tensorflow.python.tools import strip_unused_lib\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.platform import gfile\n",
    "input_node_names = ['Preprocessor/sub']\n",
    "output_node_names = ['concat', 'concat_1']\n",
    "gdef = strip_unused_lib.strip_unused(\n",
    "        input_graph_def = original_gdef,\n",
    "        input_node_names = input_node_names,\n",
    "        output_node_names = output_node_names,\n",
    "        placeholder_type_enum = dtypes.float32.as_datatype_enum)\n",
    "# Save the feature extractor to an output file\n",
    "frozen_model_file = MOBILENET_EGOHANDS_HOME + \"/\" + MODEL_NAME + '/ssd_mobilenet_v1_egohands_feature_extractor.pb'\n",
    "with gfile.GFile(frozen_model_file, \"wb\") as f:\n",
    "    f.write(gdef.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a TF model ready to be converted to CoreML\n",
    "import tfcoreml\n",
    "# Supply a dictionary of input tensors' name and shape (with # batch axis)\n",
    "input_tensor_shapes = {\"Preprocessor/sub:0\":[1,300,300,3]} # batch size is 1\n",
    "# Output CoreML model path\n",
    "coreml_model_file = MOBILENET_EGOHANDS_HOME + '/ssd_mobilenet_v1_egohands_feature_extractor.mlmodel'\n",
    "# The TF model's ouput tensor name\n",
    "output_tensor_names = ['concat:0', 'concat_1:0']\n",
    "\n",
    "# Call the converter. This may take a while\n",
    "coreml_model = tfcoreml.convert(\n",
    "        tf_model_path=frozen_model_file,\n",
    "        mlmodel_path=coreml_model_file,\n",
    "        input_name_shape_dict=input_tensor_shapes,\n",
    "        output_feature_names=output_tensor_names,\n",
    "        image_input_names=['Preprocessor/sub:0'],\n",
    "        image_scale=2./255.,\n",
    "        red_bias=-1.0,\n",
    "        green_bias=-1.0,\n",
    "        blue_bias=-1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Train MobileNetv1 with MS-COCO to generate CoreML model\n",
    "\n",
    "### Check tensorflow model functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $TF_OBJECT_DETECTION_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = SAMPLE_HOME +\"/\" + MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = MSCOCO_HOME + \"/data/mscoco_label_map.pbtxt\"\n",
    "\n",
    "NUM_CLASSES = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of simplicity we will use only 2 images:\n",
    "# image1.jpg\n",
    "# image2.jpg\n",
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "PATH_TO_TEST_IMAGES_DIR = MOBILENET_COCO_HOME + '/test_images'\n",
    "TEST_IMAGE_PATHS = [ PATH_TO_TEST_IMAGES_DIR + '/image{}.jpg'.format(i) for i in range(1, 3) ]\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "<img src=\"sample/ssd_mobilenet_v1_300_coco/images/image1.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/ssd_mobilenet_v1_300_coco/images/image2.jpg\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tensorflow model to CoreML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TF graph definition\n",
    "tf_model_path = PATH_TO_CKPT\n",
    "with open(tf_model_path, 'rb') as f:\n",
    "    serialized = f.read()\n",
    "tf.reset_default_graph()\n",
    "original_gdef = tf.GraphDef()\n",
    "original_gdef.ParseFromString(serialized)\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    tf.import_graph_def(original_gdef, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full MobileNet-SSD TF model contains 4 subgraphs: *Preprocessor*, *FeatureExtractor*, *MultipleGridAnchorGenerator*, and *Postprocessor*. Here we will extract the *FeatureExtractor* from the model and strip off the other subgraphs, as these subgraphs contain structures not currently supported in CoreML. The tasks in *Preprocessor*, *MultipleGridAnchorGenerator* and *Postprocessor* subgraphs can be achieved by other means, although they are non-trivial.\n",
    "\n",
    "By inspecting TensorFlow GraphDef, it can be found that:\n",
    "(1) the input tensor of MobileNet-SSD Feature Extractor is `Preprocessor/sub:0` of shape `(1,300,300,3)`, which contains the preprocessed image.\n",
    "(2) The output tensors are: `concat:0` of shape `(1,1917,4)`, the box coordinate encoding for each of the 1917 anchor boxes; and `concat_1:0` of shape `(1,1917,91)`, the confidence scores (logits) for each of the 91 object classes (including 1 class for background), for each of the 1917 anchor boxes.\n",
    "So we extract the feature extractor out as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip unused subgraphs and save it as another frozen TF model\n",
    "from tensorflow.python.tools import strip_unused_lib\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.platform import gfile\n",
    "input_node_names = ['Preprocessor/sub']\n",
    "output_node_names = ['concat', 'concat_1']\n",
    "gdef = strip_unused_lib.strip_unused(\n",
    "        input_graph_def = original_gdef,\n",
    "        input_node_names = input_node_names,\n",
    "        output_node_names = output_node_names,\n",
    "        placeholder_type_enum = dtypes.float32.as_datatype_enum)\n",
    "# Save the feature extractor to an output file\n",
    "frozen_model_file = 'ssd_mobilenet_v1_feature_extractor.pb'\n",
    "with gfile.GFile(frozen_model_file, \"wb\") as f:\n",
    "    f.write(gdef.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a TF model ready to be converted to CoreML\n",
    "import tfcoreml\n",
    "# Supply a dictionary of input tensors' name and shape (with # batch axis)\n",
    "input_tensor_shapes = {\"Preprocessor/sub:0\":[1,300,300,3]} # batch size is 1\n",
    "# Output CoreML model path\n",
    "coreml_model_file = MOBILENET_COCO_HOME + '/ssd_mobilenet_v1_feature_extractor.mlmodel'\n",
    "# The TF model's ouput tensor name\n",
    "output_tensor_names = ['concat:0', 'concat_1:0']\n",
    "\n",
    "# Call the converter. This may take a while\n",
    "coreml_model = tfcoreml.convert(\n",
    "        tf_model_path=frozen_model_file,\n",
    "        mlmodel_path=coreml_model_file,\n",
    "        input_name_shape_dict=input_tensor_shapes,\n",
    "        output_feature_names=output_tensor_names,\n",
    "        image_input_names=['Preprocessor/sub:0'],\n",
    "        image_scale=2./255.,\n",
    "        red_bias=-1.0,\n",
    "        green_bias=-1.0,\n",
    "        blue_bias=-1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Train YOLOv2 with MS-COCO to generate CoreML model\n",
    "\n",
    "### Convert pre-trained darknet weight to tensorflow model\n",
    "> **Note**: You don't have to do below things. All these things I have done in this docker image. Just run below cell to do it.\n",
    "\n",
    "1. Download the pre-trained darknet [yolov2 weight](https://pjreddie.com/media/files/yolov2.weights) and rename to bin/yolo.weights.\n",
    "    \n",
    "1. Download the [yolov2 configuration file](https://github.com/pjreddie/darknet/blob/master/cfg/yolov2.cfg) and rename to cfg/yolo.cfg.\n",
    "    - Change width and height to 608\n",
    "```\n",
    "    -- before\n",
    "        width=416\n",
    "        height=416\n",
    "    -- after\n",
    "        width=608\n",
    "        height=608\n",
    "```\n",
    "\n",
    "1. Use the following command to convert the weights to tensorflow pb file\n",
    "```        \n",
    "    flow --imgdir sample_img/ --model cfg/yolo.cfg --load bin/yolo.weights --json\n",
    "```\n",
    "\n",
    "Run below cells to start to transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $YOLOV2_COCO_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp loader.py $DARKFLOW_HOME/darkflow/utils\n",
    "\n",
    "!flow --model cfg/yolo.cfg --load bin/yolo.weights --savepb --verbalise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check tensorflow model functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!flow --pbLoad built_graph/yolo.pb --metaLoad built_graph/yolo.meta --imgdir sample_img/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "\n",
    "fig=plt.figure(figsize=(32, 32))\n",
    "columns = 1\n",
    "rows = 6\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.imshow(mpimg.imread(YOLOV2_COCO_HOME + \"/sample_img/out/dog.jpg\"))\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "plt.imshow(mpimg.imread(YOLOV2_COCO_HOME + \"/sample_img/out/eagle.jpg\"))\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "plt.imshow(mpimg.imread(YOLOV2_COCO_HOME + \"/sample_img/out/giraffe.jpg\"))\n",
    "fig.add_subplot(rows, columns, 4)\n",
    "plt.imshow(mpimg.imread(YOLOV2_COCO_HOME + \"/sample_img/out/horses.jpg\"))\n",
    "fig.add_subplot(rows, columns, 5)\n",
    "plt.imshow(mpimg.imread(YOLOV2_COCO_HOME + \"/sample_img/out/person.jpg\"))\n",
    "fig.add_subplot(rows, columns, 6)\n",
    "plt.imshow(mpimg.imread(YOLOV2_COCO_HOME + \"/sample_img/out/scream.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "<img src=\"sample/yolov2_608_coco/images/dog.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/yolov2_608_coco/images/eagle.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/yolov2_608_coco/images/giraffe.jpg\" width=\"300\"/>\n",
    "</p>\n",
    "<p float=\"left\">\n",
    "<img src=\"sample/yolov2_608_coco/images/horses.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/yolov2_608_coco/images/person.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/yolov2_608_coco/images/scream.jpg\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Convert tensorflow model to CoreML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $YOLOV2_COCO_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfcoreml as tf_converter\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output the graph\n",
    "\n",
    "In this step we just want to know the exact name of input and output nodes in the tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    # We load the protobuf file from the disk and parse it to retrieve the \n",
    "    # unserialized graph_def\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # Then, we import the graph_def into a new Graph and return it \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def, name=\"\")\n",
    "    return graph\n",
    "\n",
    "graph = load_graph('built_graph/yolo.pb')\n",
    "# for op in graph.get_operations(): \n",
    "#     print (op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to mlmodel format\n",
    "\n",
    "In the previose step, we know the output and input node names. And we can also get the input shape size from the cfg file. We specify these in the convert procedure and save the mlmodel file as yolo.mlmodel ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = tf_converter.convert(tf_model_path = 'built_graph/yolo.pb',\n",
    "                                     mlmodel_path = 'yolo.mlmodel',\n",
    "                             output_feature_names = ['output:0'],  # the output node name we get from the previouse step\n",
    "                                 image_input_names= ['input:0'],   # CoreML allows image as the input, the only thing we need to do is to set which node is the image input node \n",
    "                            input_name_shape_dict = {'input:0' : [1, 608, 608, 3]},  # the input node name we get from the previous step, and check the cfg file to know the exact input shape size\n",
    "                                   is_bgr = True,   # the channel order is by BGR instead of RGB\n",
    "                                   image_scale = 1 / 255.0)\t # the weights is already normalized in the range from 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Train Tiny YOLO with MS-COCO to generate CoreML model\n",
    "\n",
    "### Transfer pre-trained darknet weight to tensorflow format\n",
    "> **Note**: You don't have to do below things. All these things I have done in this docker image. Just run below cell to do it.\n",
    "\n",
    "1. Download the pre-trained darknet [yolov2-tiny weight](https://pjreddie.com/media/files/yolov2-tiny.weights) and rename to bin/tiny-yolo.weights.\n",
    "\n",
    "1. Download the [yolov2-tiny configuration file](https://github.com/pjreddie/darknet/blob/master/cfg/yolov2-tiny.cfg) and rename to cfg/tiny-yolo.cfg.\n",
    "    - Change width and height to 608\n",
    "```\n",
    "    -- before\n",
    "        width=416\n",
    "        height=416\n",
    "    -- after\n",
    "        width=608\n",
    "        height=608\n",
    "```\n",
    "\n",
    "1. Modify darkflow/darkflow/utils/loader.py to fix build error\n",
    "```\n",
    "    -- before\n",
    "        self.offset = 16\n",
    "    -- after    \n",
    "        self.offset = 20\n",
    "```\n",
    "\n",
    "1. Use the following command to convert the weights to tensorflow pb file   \n",
    "```\n",
    "    flow --imgdir sample_img/ --model cfg/tiny-yolo.cfg --load bin/tiny-yolo.weights --json\n",
    "```\n",
    "\n",
    "Run below cells to start to transfer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $YOLOV2_TINY_COCO_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp loader.py $DARKFLOW_HOME/darkflow/utils\n",
    "\n",
    "!flow --model cfg/tiny-yolo.cfg --load bin/tiny-yolo.weights --savepb --verbalise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check tensorflow model functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!flow --pbLoad built_graph/tiny-yolo.pb --metaLoad built_graph/tiny-yolo.meta --imgdir sample_img/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "\n",
    "fig=plt.figure(figsize=(32, 32))\n",
    "columns = 1\n",
    "rows = 6\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.imshow(mpimg.imread(YOLOV2_TINY_COCO_HOME + \"/sample_img/out/dog.jpg\"))\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "plt.imshow(mpimg.imread(YOLOV2_TINY_COCO_HOME + \"/sample_img/out/eagle.jpg\"))\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "plt.imshow(mpimg.imread(YOLOV2_TINY_COCO_HOME + \"/sample_img/out/giraffe.jpg\"))\n",
    "fig.add_subplot(rows, columns, 4)\n",
    "plt.imshow(mpimg.imread(YOLOV2_TINY_COCO_HOME + \"/sample_img/out/horses.jpg\"))\n",
    "fig.add_subplot(rows, columns, 5)\n",
    "plt.imshow(mpimg.imread(YOLOV2_TINY_COCO_HOME + \"/sample_img/out/person.jpg\"))\n",
    "fig.add_subplot(rows, columns, 6)\n",
    "plt.imshow(mpimg.imread(YOLOV2_TINY_COCO_HOME + \"/sample_img/out/scream.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "<img src=\"sample/yolov2_tiny_608_coco/images/dog.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/yolov2_tiny_608_coco/images/eagle.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/yolov2_tiny_608_coco/images/giraffe.jpg\" width=\"300\"/>\n",
    "</p>\n",
    "<p float=\"left\">\n",
    "<img src=\"sample/yolov2_tiny_608_coco/images/horses.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/yolov2_tiny_608_coco/images/person.jpg\" width=\"300\"/>\n",
    "<img src=\"sample/yolov2_tiny_608_coco/images/scream.jpg\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tensorflow weight to CoreML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $YOLOV2_TINY_COCO_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfcoreml as tf_converter\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output the graph\n",
    "\n",
    "In this step we just want to know the exact name of input and output nodes in the tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    # We load the protobuf file from the disk and parse it to retrieve the \n",
    "    # unserialized graph_def\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # Then, we import the graph_def into a new Graph and return it \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def, name=\"\")\n",
    "    return graph\n",
    "\n",
    "graph = load_graph('built_graph/tiny-yolo.pb')\n",
    "# for op in graph.get_operations(): \n",
    "#     print (op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to mlmodel format\n",
    "\n",
    "In the previose step, we know the output and input node names. And we can also get the input shape size from the cfg file. We specify these in the convert procedure and save the mlmodel file as tiny_yolo.mlmodel ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = tf_converter.convert(tf_model_path = 'built_graph/tiny-yolo.pb',\n",
    "                                     mlmodel_path = 'tiny_yolo.mlmodel',\n",
    "                             output_feature_names = ['output:0'],  # the output node name we get from the previouse step\n",
    "                                 image_input_names= ['input:0'],   # CoreML allows image as the input, the only thing we need to do is to set which node is the image input node \n",
    "                            input_name_shape_dict = {'input:0' : [1, 608, 608, 3]},  # the input node name we get from the previous step, and check the cfg file to know the exact input shape size\n",
    "                                   is_bgr = True,   # the channel order is by BGR instead of RGB\n",
    "                                   image_scale = 1 / 255.0)\t # the weights is already normalized in the range from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
